{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 基于 ResNet 的多模态医学图像分割入门教程\n\n> 目标：给完全新手的**可跑通**教程。我们将从零开始：准备数据 → 定义数据集 → 构建模型 → 训练 → 评估 → 推理。\n\n本 Notebook 面向**多模态**医学数据：\n- **图像**（如 CT/MRI/超声）\n- **结构化元数据**（如年龄、性别、模态类型、病灶大小）\n\n我们先用“可跑通的小样例数据”，再说明如何替换成你自己的真实数据。\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0. 环境准备（只需运行一次）\n\n如果你在新的环境里，请先安装依赖：\n\n```bash\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\npip install numpy pandas pillow matplotlib tqdm\n```\n\n> 如果你有 GPU，请把 `--index-url` 换成对应 CUDA 版本。\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. 项目与数据结构（非常重要）\n\n我们建议你把数据按如下结构摆放：\n\n```\nexamples/transformer_tutorial2/\n  resnet_multimodal_seg.ipynb\n  data/\n    resnet_tutorial/\n      images/      # 原始图像\n      masks/       # 对应的分割mask（0/1 或 0~N 类）\n      metadata.csv # 结构化元数据（每行对应一张图像）\n```\n\n`metadata.csv` 至少包含：\n- `image_id`：文件名（不带扩展名）\n- `age`、`sex`、`modality` 等你想用的字段\n\n下面先生成一个**可运行的演示数据集**。\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageDraw\n\nROOT = Path('examples/transformer_tutorial2/data/resnet_tutorial')\nIMG_DIR = ROOT / 'images'\nMSK_DIR = ROOT / 'masks'\nROOT.mkdir(parents=True, exist_ok=True)\nIMG_DIR.mkdir(parents=True, exist_ok=True)\nMSK_DIR.mkdir(parents=True, exist_ok=True)\n\n# 生成一个小的可跑通数据集（20 张 128x128 图像）\nrows = []\nfor i in range(20):\n    img = Image.new('L', (128, 128), color=0)\n    mask = Image.new('L', (128, 128), color=0)\n\n    draw_img = ImageDraw.Draw(img)\n    draw_msk = ImageDraw.Draw(mask)\n\n    # 随机圆形“病灶”\n    cx, cy = np.random.randint(32, 96, size=2)\n    r = np.random.randint(10, 25)\n    bbox = (cx - r, cy - r, cx + r, cy + r)\n    draw_img.ellipse(bbox, fill=200)\n    draw_msk.ellipse(bbox, fill=255)\n\n    image_id = f'sample_{i:03d}'\n    img.save(IMG_DIR / f'{image_id}.png')\n    mask.save(MSK_DIR / f'{image_id}.png')\n\n    rows.append({\n        'image_id': image_id,\n        'age': int(np.random.randint(18, 80)),\n        'sex': int(np.random.randint(0, 2)),  # 0/1\n        'modality': int(np.random.randint(0, 3)),  # 0:CT,1:MR,2:US\n    })\n\nmeta = pd.DataFrame(rows)\nmeta.to_csv(ROOT / 'metadata.csv', index=False)\nmeta.head()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. 定义数据集（图像 + mask + 元数据）\n\n我们将图像和 mask 读入，并把元数据转成数值向量。**这一步是多模态的关键。**\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nclass MultimodalSegDataset(Dataset):\n    def __init__(self, root_dir, img_size=128):\n        self.root_dir = Path(root_dir)\n        self.img_dir = self.root_dir / 'images'\n        self.msk_dir = self.root_dir / 'masks'\n        self.meta = pd.read_csv(self.root_dir / 'metadata.csv')\n\n        self.transform = transforms.Compose([\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor(),  # [0,1]\n        ])\n\n    def __len__(self):\n        return len(self.meta)\n\n    def __getitem__(self, idx):\n        row = self.meta.iloc[idx]\n        image_id = row['image_id']\n\n        # 读取图像（灰度）\n        img = Image.open(self.img_dir / f'{image_id}.png').convert('L')\n        # 读取 mask（灰度）\n        mask = Image.open(self.msk_dir / f'{image_id}.png').convert('L')\n\n        img = self.transform(img)  # [1,H,W]\n        mask = self.transform(mask)\n        mask = (mask > 0.5).float()  # 二值化\n\n        # 元数据向量：age/sex/modality -> float 张量\n        # 把元数据拼成向量\n        meta = torch.tensor([row['age'], row['sex'], row['modality']], dtype=torch.float32)\n        # 简单归一化（实际项目建议使用训练集均值/方差）\n        meta[0] = meta[0] / 100.0\n        meta[2] = meta[2] / 2.0\n\n        return img, mask, meta\n\nDATASET_ROOT = 'examples/transformer_tutorial2/data/resnet_tutorial'\ndataset = MultimodalSegDataset(DATASET_ROOT)\nimg, mask, meta = dataset[0]\nimg.shape, mask.shape, meta\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. 构建模型：ResNet 编码器 + 轻量解码器（带元数据注入）\n\n思路：\n1. 用 ResNet18 把图像编码为特征图。\n2. 用元数据做一个小 MLP，得到与特征通道匹配的向量。\n3. **把元数据向量加到特征图上**（即“多模态融合”）。\n4. 用解码器上采样回到原图大小，输出分割 mask。\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import torch.nn as nn\nimport torchvision.models as models\n\nclass ResNetSeg(nn.Module):\n    def __init__(self, meta_dim=3):\n        super().__init__()\n        backbone = models.resnet18(weights=None)\n        self.encoder = nn.Sequential(*list(backbone.children())[:-2])  # [B,512,4,4] for 128x128\n\n        self.meta_mlp = nn.Sequential(\n            nn.Linear(meta_dim, 64), nn.ReLU(),\n            nn.Linear(64, 512)\n        )\n\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(512, 256, 2, 2),  # 8x8\n            nn.ReLU(),\n            nn.ConvTranspose2d(256, 128, 2, 2),  # 16x16\n            nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, 2, 2),   # 32x32\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 32, 2, 2),    # 64x64\n            nn.ReLU(),\n            nn.ConvTranspose2d(32, 16, 2, 2),    # 128x128\n            nn.ReLU(),\n            nn.Conv2d(16, 1, kernel_size=1)\n        )\n\n    def forward(self, x, meta):\n        feat = self.encoder(x)\n        meta_embed = self.meta_mlp(meta).unsqueeze(-1).unsqueeze(-1)\n        feat = feat + meta_embed  # 多模态融合\n        logits = self.decoder(feat)\n        return logits\n\nmodel = ResNetSeg()\nmodel(img.unsqueeze(0), meta.unsqueeze(0)).shape\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3.1 快速运行自检（确保模型能前向）\n\n这一小段不会训练，只做**快速检查**：\n- 模型能前向\n- 输出尺寸正确\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# ✅ 快速自检：模型前向 + 输出尺寸\nimg, mask, meta = dataset[0]\nmodel = ResNetSeg()\nlogits = model(img.unsqueeze(0), meta.unsqueeze(0))\nprint('logits shape:', logits.shape)\nassert logits.shape[-2:] == mask.shape[-2:], '输出尺寸应与 mask 一致'\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. 训练准备\n\n我们使用：\n- 损失函数：`BCE + Dice`（分割常用）\n- 优化器：Adam\n- 指标：Dice 系数、IoU\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from torch.optim import Adam\n\ndef dice_loss(logits, targets, eps=1e-6):\n    probs = torch.sigmoid(logits)\n    num = 2 * (probs * targets).sum(dim=(1,2,3))\n    den = probs.sum(dim=(1,2,3)) + targets.sum(dim=(1,2,3)) + eps\n    return 1 - (num / den).mean()\n\ndef iou_score(logits, targets, eps=1e-6):\n    probs = (torch.sigmoid(logits) > 0.5).float()\n    inter = (probs * targets).sum(dim=(1,2,3))\n    union = (probs + targets - probs*targets).sum(dim=(1,2,3))\n    return ((inter + eps) / (union + eps)).mean().item()\n\nloader = DataLoader(dataset, batch_size=4, shuffle=True)\nmodel = ResNetSeg()\noptimizer = Adam(model.parameters(), lr=1e-3)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. 训练循环（最小可运行版）\n\n> 这里只跑 2 个 epoch，确保你能“快速跑通”。\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from tqdm import tqdm\n\nmodel.train()\nfor epoch in range(2):\n    epoch_loss = 0\n    for imgs, masks, metas in tqdm(loader, desc=f'Epoch {epoch+1}'):\n        # 前向传播\n        logits = model(imgs, metas)\n        # 计算损失（BCE + Dice）\n        loss = nn.functional.binary_cross_entropy_with_logits(logits, masks) + dice_loss(logits, masks)\n\n        # 反向传播与更新参数\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n    print('avg loss:', epoch_loss / len(loader))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. 评估与可视化\n\n我们看看单张图像的预测效果（可视化）。\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import matplotlib.pyplot as plt\n\nmodel.eval()\nwith torch.no_grad():\n    img, mask, meta = dataset[0]\n    pred = torch.sigmoid(model(img.unsqueeze(0), meta.unsqueeze(0)))[0,0]\n\n    plt.figure(figsize=(8,3))\n    plt.subplot(1,3,1); plt.title('Image'); plt.imshow(img[0], cmap='gray')\n    plt.subplot(1,3,2); plt.title('GT Mask'); plt.imshow(mask[0], cmap='gray')\n    plt.subplot(1,3,3); plt.title('Pred'); plt.imshow(pred, cmap='gray')\n    plt.show()\n\n    print('IoU:', iou_score(pred.unsqueeze(0).unsqueeze(0), mask.unsqueeze(0)))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. 用你自己的数据怎么做？\n\n### 7.1 数据摆放\n把你的数据替换到：\n```\nexamples/transformer_tutorial2/data/resnet_tutorial/\n  images/\n  masks/\n  metadata.csv\n```\n\n### 7.2 需要注意的点\n1. **图像和 mask 必须一一对应**。\n2. `metadata.csv` 的 `image_id` 要和文件名一致（不带扩展名）。\n3. 如果你有多个模态（CT/MRI/US），可以把它当成一个数值类别（0/1/2）。\n\n### 7.3 预训练与微调\n- **预训练**：可以先在公开数据集（如 ACDC、BraTS）上训练。\n- **微调**：把学习率调小（如 1e-4），训练轮数调少，继续在你的私有数据上训练。\n\n### 7.4 进一步提升\n- 替换更强的解码器（如 U-Net / FPN / DeepLab）。\n- 加入更丰富的元数据（文本报告、临床指标）。\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}