{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 基于 ViT 的多模态医学图像分割入门教程\n\n> 目标：用 **Vision Transformer (ViT)** 做医学分割，并融合结构化元数据。\n\n流程与 ResNet 版本一致：\n1. 准备数据\n2. 定义数据集\n3. 构建模型（ViT 编码器 + 解码器）\n4. 训练\n5. 评估\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0. 环境准备（只需运行一次）\n\n```bash\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\npip install timm numpy pandas pillow matplotlib tqdm\n```\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. 项目与数据结构\n\n与 ResNet 版本一致：\n```\nexamples/transformer_tutorial2/\n  vit_multimodal_seg.ipynb\n  data/\n    vit_tutorial/\n      images/\n      masks/\n      metadata.csv\n```\n\n下面先生成一个**可跑通的小数据集**。\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageDraw\n\nROOT = Path('examples/transformer_tutorial2/data/vit_tutorial')\nIMG_DIR = ROOT / 'images'\nMSK_DIR = ROOT / 'masks'\nROOT.mkdir(parents=True, exist_ok=True)\nIMG_DIR.mkdir(parents=True, exist_ok=True)\nMSK_DIR.mkdir(parents=True, exist_ok=True)\n\nrows = []\nfor i in range(20):\n    img = Image.new('L', (224, 224), color=0)\n    mask = Image.new('L', (224, 224), color=0)\n\n    draw_img = ImageDraw.Draw(img)\n    draw_msk = ImageDraw.Draw(mask)\n\n    cx, cy = np.random.randint(50, 174, size=2)\n    r = np.random.randint(15, 40)\n    bbox = (cx - r, cy - r, cx + r, cy + r)\n    draw_img.ellipse(bbox, fill=200)\n    draw_msk.ellipse(bbox, fill=255)\n\n    image_id = f'sample_{i:03d}'\n    img.save(IMG_DIR / f'{image_id}.png')\n    mask.save(MSK_DIR / f'{image_id}.png')\n\n    rows.append({\n        'image_id': image_id,\n        'age': int(np.random.randint(18, 80)),\n        'sex': int(np.random.randint(0, 2)),\n        'modality': int(np.random.randint(0, 3)),\n    })\n\nmeta = pd.DataFrame(rows)\nmeta.to_csv(ROOT / 'metadata.csv', index=False)\nmeta.head()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. 定义数据集\n\n与 ResNet 版本几乎一致，只是图像大小为 224。\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nclass MultimodalSegDataset(Dataset):\n    def __init__(self, root_dir, img_size=224):\n        self.root_dir = Path(root_dir)\n        self.img_dir = self.root_dir / 'images'\n        self.msk_dir = self.root_dir / 'masks'\n        self.meta = pd.read_csv(self.root_dir / 'metadata.csv')\n\n        self.transform = transforms.Compose([\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor(),\n        ])\n\n    def __len__(self):\n        return len(self.meta)\n\n    def __getitem__(self, idx):\n        row = self.meta.iloc[idx]\n        image_id = row['image_id']\n\n        # 读取图像（灰度）\n        img = Image.open(self.img_dir / f'{image_id}.png').convert('L')\n        # 读取 mask（灰度）\n        mask = Image.open(self.msk_dir / f'{image_id}.png').convert('L')\n\n        img = self.transform(img)\n        mask = self.transform(mask)\n        mask = (mask > 0.5).float()\n\n        # 把元数据拼成向量\n        meta = torch.tensor([row['age'], row['sex'], row['modality']], dtype=torch.float32)\n        meta[0] = meta[0] / 100.0\n        meta[2] = meta[2] / 2.0\n        return img, mask, meta\n\nDATASET_ROOT = 'examples/transformer_tutorial2/data/vit_tutorial'\ndataset = MultimodalSegDataset(DATASET_ROOT)\nimg, mask, meta = dataset[0]\nimg.shape, mask.shape, meta\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. 构建模型：ViT 编码器 + 解码器（带元数据融合）\n\n思路：\n1. ViT 把图像切成 patch，并输出 token。\n2. 去掉 CLS token，把剩余 token reshape 回特征图。\n3. 注入元数据（MLP -> 与特征通道匹配）。\n4. 解码器上采样输出 mask。\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import torch.nn as nn\nimport timm\n\nclass ViTSeg(nn.Module):\n    def __init__(self, meta_dim=3):\n        super().__init__()\n        self.vit = timm.create_model('vit_tiny_patch16_224', pretrained=False)\n        self.embed_dim = self.vit.embed_dim\n\n        self.meta_mlp = nn.Sequential(\n            nn.Linear(meta_dim, 64), nn.ReLU(),\n            nn.Linear(64, self.embed_dim)\n        )\n\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(self.embed_dim, 256, 2, 2),  # 28x28\n            nn.ReLU(),\n            nn.ConvTranspose2d(256, 128, 2, 2),  # 56x56\n            nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, 2, 2),   # 112x112\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 32, 2, 2),    # 224x224\n            nn.ReLU(),\n            nn.Conv2d(32, 1, kernel_size=1)\n        )\n\n    def forward(self, x, meta):\n        # forward_features 输出 [B, N+1, C] (含 CLS token)\n        feats = self.vit.forward_features(x)\n        tokens = feats[:, 1:, :]  # 去掉 CLS token\n        b, n, c = tokens.shape\n        h = w = int(n ** 0.5)\n        feat_map = tokens.transpose(1, 2).reshape(b, c, h, w)\n\n        meta_embed = self.meta_mlp(meta).unsqueeze(-1).unsqueeze(-1)\n        feat_map = feat_map + meta_embed\n\n        logits = self.decoder(feat_map)\n        return logits\n\nmodel = ViTSeg()\nmodel(img.unsqueeze(0), meta.unsqueeze(0)).shape\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3.1 快速运行自检（确保模型能前向）\n\n这一小段不会训练，只做**快速检查**：\n- 模型能前向\n- 输出尺寸正确\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# ✅ 快速自检：模型前向 + 输出尺寸\nimg, mask, meta = dataset[0]\nmodel = ViTSeg()\nlogits = model(img.unsqueeze(0), meta.unsqueeze(0))\nprint('logits shape:', logits.shape)\nassert logits.shape[-2:] == mask.shape[-2:], '输出尺寸应与 mask 一致'\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. 训练准备（损失 + 指标）\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from torch.optim import Adam\n\ndef dice_loss(logits, targets, eps=1e-6):\n    probs = torch.sigmoid(logits)\n    num = 2 * (probs * targets).sum(dim=(1,2,3))\n    den = probs.sum(dim=(1,2,3)) + targets.sum(dim=(1,2,3)) + eps\n    return 1 - (num / den).mean()\n\ndef iou_score(logits, targets, eps=1e-6):\n    probs = (torch.sigmoid(logits) > 0.5).float()\n    inter = (probs * targets).sum(dim=(1,2,3))\n    union = (probs + targets - probs*targets).sum(dim=(1,2,3))\n    return ((inter + eps) / (union + eps)).mean().item()\n\nloader = DataLoader(dataset, batch_size=2, shuffle=True)\nmodel = ViTSeg()\noptimizer = Adam(model.parameters(), lr=1e-3)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. 训练循环（最小可运行版）\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from tqdm import tqdm\n\nmodel.train()\nfor epoch in range(2):\n    epoch_loss = 0\n    for imgs, masks, metas in tqdm(loader, desc=f'Epoch {epoch+1}'):\n        # 前向传播\n        logits = model(imgs, metas)\n        # 计算损失（BCE + Dice）\n        loss = nn.functional.binary_cross_entropy_with_logits(logits, masks) + dice_loss(logits, masks)\n\n        # 反向传播与更新参数\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n    print('avg loss:', epoch_loss / len(loader))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. 评估与可视化\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import matplotlib.pyplot as plt\n\nmodel.eval()\nwith torch.no_grad():\n    img, mask, meta = dataset[0]\n    pred = torch.sigmoid(model(img.unsqueeze(0), meta.unsqueeze(0)))[0,0]\n\n    plt.figure(figsize=(8,3))\n    plt.subplot(1,3,1); plt.title('Image'); plt.imshow(img[0], cmap='gray')\n    plt.subplot(1,3,2); plt.title('GT Mask'); plt.imshow(mask[0], cmap='gray')\n    plt.subplot(1,3,3); plt.title('Pred'); plt.imshow(pred, cmap='gray')\n    plt.show()\n\n    print('IoU:', iou_score(pred.unsqueeze(0).unsqueeze(0), mask.unsqueeze(0)))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. 用你自己的数据怎么做？\n\n与 ResNet 版本相同：\n1. 替换 `images/` 和 `masks/`。\n2. 更新 `metadata.csv`。\n3. 如果图像大小不是 224，可以修改 `img_size` 和 ViT 模型（patch size 必须整除图像大小）。\n\n### 预训练/微调建议\n- 可以先用大数据训练 ViT，再在你的数据上微调。\n- 微调时可冻结前几层，减少过拟合。\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}