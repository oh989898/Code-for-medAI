{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 基于 SAM 的多模态医学图像分割入门教程\n\n> SAM（Segment Anything Model）通常使用“提示（prompt）”进行分割。\n\n本教程教你：\n1. 如何准备数据（图像 + mask + 元数据）\n2. 如何把 mask 转成 prompt（例如 bbox）\n3. 如何微调 SAM 的 mask decoder\n\n**注意**：SAM 很大，真正训练需要 GPU。为了可跑通，我们做一个**极简可运行版**。\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0. 环境准备\n\n```bash\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\npip install numpy pandas pillow matplotlib tqdm\npip install git+https://github.com/facebookresearch/segment-anything.git\n```\n\n下载 SAM 权重（任选一个）：\n```bash\n# 这里示例 vit_b，文件较小\nwget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth -O sam_vit_b.pth\n```\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. 数据结构\n\n```\nexamples/transformer_tutorial2/\n  sam_multimodal_seg.ipynb\n  data/\n    sam_tutorial/\n      images/\n      masks/\n      metadata.csv\n```\n\n我们先创建一个小样例数据集。\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageDraw\n\nROOT = Path('examples/transformer_tutorial2/data/sam_tutorial')\nIMG_DIR = ROOT / 'images'\nMSK_DIR = ROOT / 'masks'\nROOT.mkdir(parents=True, exist_ok=True)\nIMG_DIR.mkdir(parents=True, exist_ok=True)\nMSK_DIR.mkdir(parents=True, exist_ok=True)\n\nrows = []\nfor i in range(10):\n    img = Image.new('RGB', (256, 256), color=(0, 0, 0))\n    mask = Image.new('L', (256, 256), color=0)\n\n    draw_img = ImageDraw.Draw(img)\n    draw_msk = ImageDraw.Draw(mask)\n\n    cx, cy = np.random.randint(60, 196, size=2)\n    r = np.random.randint(20, 45)\n    bbox = (cx - r, cy - r, cx + r, cy + r)\n    draw_img.ellipse(bbox, fill=(180, 180, 180))\n    draw_msk.ellipse(bbox, fill=255)\n\n    image_id = f'sample_{i:03d}'\n    img.save(IMG_DIR / f'{image_id}.png')\n    mask.save(MSK_DIR / f'{image_id}.png')\n\n    rows.append({\n        'image_id': image_id,\n        'age': int(np.random.randint(18, 80)),\n        'sex': int(np.random.randint(0, 2)),\n        'modality': int(np.random.randint(0, 3)),\n    })\n\nmeta = pd.DataFrame(rows)\nmeta.to_csv(ROOT / 'metadata.csv', index=False)\nmeta.head()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. 数据集与 prompt（bbox）\n\nSAM 的输入除了图像，还需要 prompt（比如 bbox 或点）。\n我们从 mask 中计算 bbox，作为训练 prompt。\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nclass SamSegDataset(Dataset):\n    def __init__(self, root_dir, img_size=256):\n        self.root_dir = Path(root_dir)\n        self.img_dir = self.root_dir / 'images'\n        self.msk_dir = self.root_dir / 'masks'\n        self.meta = pd.read_csv(self.root_dir / 'metadata.csv')\n\n        self.transform = transforms.Compose([\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor(),\n        ])\n\n    def __len__(self):\n        return len(self.meta)\n\n    def __getitem__(self, idx):\n        row = self.meta.iloc[idx]\n        image_id = row['image_id']\n\n        # 读取图像（RGB）\n        img = Image.open(self.img_dir / f'{image_id}.png').convert('RGB')\n        # 读取 mask（灰度）\n        mask = Image.open(self.msk_dir / f'{image_id}.png').convert('L')\n\n        img = self.transform(img)\n        mask = self.transform(mask)\n        mask = (mask > 0.5).float()\n\n        # 计算 bbox（x0,y0,x1,y1）\n        # 从 mask 里找出前景像素，计算 bbox\n        ys, xs = torch.where(mask[0] > 0)\n        x0, x1 = xs.min().item(), xs.max().item()\n        y0, y1 = ys.min().item(), ys.max().item()\n        bbox = torch.tensor([x0, y0, x1, y1], dtype=torch.float32)\n\n        # 把元数据拼成向量\n        meta = torch.tensor([row['age'], row['sex'], row['modality']], dtype=torch.float32)\n        meta[0] = meta[0] / 100.0\n        meta[2] = meta[2] / 2.0\n\n        return img, mask, bbox, meta\n\nDATASET_ROOT = 'examples/transformer_tutorial2/data/sam_tutorial'\ndataset = SamSegDataset(DATASET_ROOT)\nimg, mask, bbox, meta = dataset[0]\nimg.shape, mask.shape, bbox, meta\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2.1 快速运行自检（确保数据与 prompt 正常）\n\n这一小段不会训练，只做**快速检查**：\n- 数据能读取\n- prompt (bbox) 正常\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# ✅ 快速自检：数据 + bbox 生成\nimg, mask, bbox, meta = dataset[0]\nprint('image shape:', img.shape)\nprint('bbox:', bbox.tolist())\nassert bbox.numel() == 4, 'bbox 需要 4 个值 (x0,y0,x1,y1)'\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. 构建 SAM 微调模型（只微调 mask decoder）\n\nSAM 结构非常大，我们做一个“轻量微调”：\n- 冻结图像编码器与 prompt 编码器\n- 只训练 mask decoder\n\n同时，我们加入一个简单的元数据嵌入，把它加到 mask decoder 的输出上。\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import torch.nn as nn\nfrom pathlib import Path\nfrom segment_anything import sam_model_registry\nfrom segment_anything.utils.transforms import ResizeLongestSide\n\nclass SamWithMeta(nn.Module):\n    def __init__(self, checkpoint_path=None, meta_dim=3, model_type='vit_b'):\n        super().__init__()\n        self.sam = sam_model_registry[model_type](checkpoint=checkpoint_path)\n        self.transform = ResizeLongestSide(self.sam.image_encoder.img_size)\n\n        # 冻结编码器，只训练 mask decoder\n        for p in self.sam.image_encoder.parameters():\n            p.requires_grad = False\n        for p in self.sam.prompt_encoder.parameters():\n            p.requires_grad = False\n\n        self.meta_mlp = nn.Sequential(\n            nn.Linear(meta_dim, 64), nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n\n    def forward(self, image, bbox, meta):\n        # image: [B,3,H,W] -> SAM 需要先 resize/pad 到 1024\n        if bbox.dim() == 1:\n            bbox = bbox.unsqueeze(0)\n\n        # resize image 和 box\n        resized_image = self.transform.apply_image_torch(image)\n        resized_image = self.sam.preprocess(resized_image)\n\n        resized_boxes = self.transform.apply_boxes_torch(bbox, image.shape[-2:])\n\n        # 编码图像和 prompt\n        image_embedding = self.sam.image_encoder(resized_image)\n        sparse_embeddings, dense_embeddings = self.sam.prompt_encoder(\n            points=None,\n            boxes=resized_boxes,\n            masks=None,\n        )\n\n        # mask decoder\n        low_res_masks, _ = self.sam.mask_decoder(\n            image_embeddings=image_embedding,\n            image_pe=self.sam.prompt_encoder.get_dense_pe(),\n            sparse_prompt_embeddings=sparse_embeddings,\n            dense_prompt_embeddings=dense_embeddings,\n            multimask_output=False,\n        )\n\n        # 结合元数据：学一个 bias 加到 logits 上\n        meta_bias = self.meta_mlp(meta).view(-1, 1, 1, 1)\n        low_res_masks = low_res_masks + meta_bias\n        return low_res_masks\n\nCHECKPOINT = Path('sam_vit_b.pth')\ncheckpoint_path = CHECKPOINT if CHECKPOINT.exists() else None\nif checkpoint_path is None:\n    print('⚠️ 未找到 sam_vit_b.pth，将使用随机初始化权重（仅用于跑通流程）。')\n\nmodel = SamWithMeta(checkpoint_path=checkpoint_path)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. 训练准备与训练循环（简化版）\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from torch.optim import Adam\n\ndef dice_loss(logits, targets, eps=1e-6):\n    probs = torch.sigmoid(logits)\n    num = 2 * (probs * targets).sum(dim=(1,2,3))\n    den = probs.sum(dim=(1,2,3)) + targets.sum(dim=(1,2,3)) + eps\n    return 1 - (num / den).mean()\n\nloader = DataLoader(dataset, batch_size=1, shuffle=True)\noptimizer = Adam(model.parameters(), lr=1e-4)\n\nmodel.train()\nfor epoch in range(1):\n    epoch_loss = 0\n    for img, mask, bbox, meta in loader:\n        # 前向传播\n        logits = model(img, bbox, meta)\n        # SAM 输出是低分辨率，需要上采样到 mask 大小\n        logits = nn.functional.interpolate(logits, size=mask.shape[-2:], mode='bilinear', align_corners=False)\n\n        # 计算损失（BCE + Dice）\n        loss = nn.functional.binary_cross_entropy_with_logits(logits, mask) + dice_loss(logits, mask)\n        # 反向传播与更新参数\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n    print('avg loss:', epoch_loss / len(loader))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. 评估与可视化\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import matplotlib.pyplot as plt\n\nmodel.eval()\nwith torch.no_grad():\n    img, mask, bbox, meta = dataset[0]\n    logits = model(img.unsqueeze(0), bbox.unsqueeze(0), meta.unsqueeze(0))\n    logits = nn.functional.interpolate(logits, size=mask.shape[-2:], mode='bilinear', align_corners=False)\n    pred = torch.sigmoid(logits)[0,0]\n\n    plt.figure(figsize=(8,3))\n    plt.subplot(1,3,1); plt.title('Image'); plt.imshow(img.permute(1,2,0))\n    plt.subplot(1,3,2); plt.title('GT Mask'); plt.imshow(mask[0], cmap='gray')\n    plt.subplot(1,3,3); plt.title('Pred'); plt.imshow(pred, cmap='gray')\n    plt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. 用你自己的数据怎么做？\n\n1. 替换 `images/`、`masks/` 和 `metadata.csv`。\n2. 如果你有**真实的 prompt**（例如医生给出的点/框），可以直接用它们。\n3. 如果你没有 prompt，可以像本教程一样从 mask 生成 bbox。\n\n### 预训练与微调\n- SAM 本身是大模型，通常只微调 mask decoder。\n- 你也可以逐步解冻 image encoder（学习率调小）。\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}