{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# åŒ»å­¦å¤šæ¨¡æ€ CLIP æ•™å­¦ï¼ˆæ–°æ‰‹å‹å¥½ç‰ˆï¼‰\n\næ¬¢è¿ï¼è¿™ä¸ª Notebook ä¼šä¸€æ­¥ä¸€æ­¥å¸¦ä½ åšä¸€ä¸ª **å¯ä»¥è·‘é€šçš„ã€åŸºäº CLIP çš„åŒ»å­¦å¤šæ¨¡æ€ï¼ˆå›¾åƒ + æ–‡æœ¬ï¼‰åˆ†æ** å°æ•™ç¨‹ã€‚\n\nä½ å°†å­¦åˆ°ï¼š\n- æ•°æ®å¦‚ä½•å‡†å¤‡ã€å¦‚ä½•å­˜æ”¾ï¼ˆé€‚ç”¨äºè‡ªå·±çš„åŒ»å­¦æ•°æ®ï¼‰\n- å¦‚ä½•åŠ è½½é¢„è®­ç»ƒ CLIP\n- å¦‚ä½•è¿›è¡Œå¾®è°ƒï¼ˆfine-tuneï¼‰\n- å¦‚ä½•è¯„ä¼°ï¼ˆç®€å•å‡†ç¡®ç‡/æ£€ç´¢ Top-1ï¼‰\n- å¦‚ä½•ä¿å­˜ä¸æ¨ç†\n\n> **å®šä½**ï¼šé¢å‘å®Œå…¨æ–°æ‰‹ï¼Œä»£ç æ³¨é‡Šéå¸¸æ¸…æ¥šï¼Œæ¯ä¸€æ­¥åœ¨åšä»€ä¹ˆéƒ½å†™æ˜ç™½ã€‚\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0. å®‰è£…ä¾èµ–ï¼ˆåªéœ€è¦åšä¸€æ¬¡ï¼‰\n\nåœ¨ç»ˆç«¯æ‰§è¡Œï¼š\n```bash\n# å»ºè®®åœ¨è™šæ‹Ÿç¯å¢ƒä¸­å®‰è£…\npip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\npip install transformers datasets pillow scikit-learn matplotlib\n```\n\n> å¦‚æœä½ æœ‰ GPUï¼Œå¯ä»¥æŠŠ PyTorch å®‰è£…ä¸º GPU ç‰ˆæœ¬ï¼ˆé€Ÿåº¦æ›´å¿«ï¼‰ã€‚\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. æ•°æ®å‡†å¤‡ï¼ˆéå¸¸å…³é”®ï¼‰\n\næˆ‘ä»¬éœ€è¦ **å›¾åƒ + æ–‡æœ¬** çš„é…å¯¹æ•°æ®ã€‚\n\n### 1.1 æ¨èçš„æ•°æ®ç›®å½•ç»“æ„\n```\nmy_med_data/\n  images/\n    0001.png\n    0002.png\n    ...\n  metadata.csv\n```\n\n### 1.2 metadata.csv æ ¼å¼\nè‡³å°‘åŒ…å«ä¸¤åˆ—ï¼š\n- `image_path`ï¼šç›¸å¯¹è·¯å¾„ï¼Œä¾‹å¦‚ `images/0001.png`\n- `report`ï¼šä¸å›¾åƒå¯¹åº”çš„æ–‡æœ¬æè¿°ï¼ˆæ¯”å¦‚æ”¾å°„ç§‘æŠ¥å‘Šï¼‰\n\nç¤ºä¾‹ï¼š\n```csv\nimage_path,report\nimages/0001.png,\"èƒ¸ç‰‡æ˜¾ç¤ºåŒè‚ºçº¹ç†å¢ç²—...\"\nimages/0002.png,\"å¿ƒå½±å¢å¤§ï¼Œè€ƒè™‘å¿ƒè¡°...\"\n```\n\n> **ä½ çš„çœŸå®æ•°æ®**ï¼šå°±æŒ‰è¿™ä¸ªæ ¼å¼æ•´ç†å³å¯ã€‚\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. ï¼ˆå¯é€‰ï¼‰ç”Ÿæˆä¸€ä¸ªå¯è·‘é€šçš„ç©å…·æ•°æ®é›†\n\nå¦‚æœä½ æš‚æ—¶æ²¡æœ‰çœŸå®æ•°æ®ï¼Œå…ˆç”¨ä¸‹é¢çš„ä»£ç ç”Ÿæˆä¸€ä¸ª **å°å‹ç¤ºä¾‹æ•°æ®**ï¼Œè¿™æ ·ä½ å¯ä»¥å¿«é€Ÿè·‘é€šæ•´ä¸ªæµç¨‹ã€‚\n\n> è¿™ä¸€æ­¥åªç”¨äºæ¼”ç¤ºï¼Œä¸ä»£è¡¨çœŸå®åŒ»å­¦ç»“æœã€‚\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import os\nfrom pathlib import Path\nfrom PIL import Image, ImageDraw\nimport csv\n\n# 1) è®¾ç½®æ•°æ®ä¿å­˜è·¯å¾„\nroot_dir = Path(\"toy_med_data\")\nimg_dir = root_dir / \"images\"\nimg_dir.mkdir(parents=True, exist_ok=True)\n\n# 2) å‡†å¤‡ä¸€äº›ç®€å•çš„â€œæŠ¥å‘Šæ–‡æœ¬â€\nreports = [\n    \"èƒ¸ç‰‡æ˜¾ç¤ºåŒè‚ºçº¹ç†å¢ç²—ï¼Œè€ƒè™‘è½»åº¦ç‚ç—‡ã€‚\",\n    \"å¿ƒå½±ç•¥å¤§ï¼Œå»ºè®®ç»“åˆä¸´åºŠè¿›ä¸€æ­¥æ£€æŸ¥ã€‚\",\n    \"å³ä¸‹è‚ºå¯è§ç‰‡çŠ¶é˜´å½±ï¼Œè€ƒè™‘æ„ŸæŸ“ã€‚\",\n    \"å·¦è‚ºé€äº®åº¦ç¨å‡ï¼Œå¯èƒ½æœ‰è½»åº¦ç§¯æ¶²ã€‚\",\n    \"è‚ºé‡æ¸…æ™°ï¼Œæœªè§æ˜æ˜¾å¼‚å¸¸ã€‚\",\n]\n\n# 3) ç”Ÿæˆç®€æ˜“å›¾ç‰‡ï¼ˆä»…ç”¨äºç¤ºä¾‹ï¼‰\nfor i, text in enumerate(reports, start=1):\n    img = Image.new(\"RGB\", (256, 256), color=(255, 255, 255))\n    draw = ImageDraw.Draw(img)\n    draw.rectangle([40, 40, 216, 216], outline=(0, 0, 0), width=3)\n    draw.text((50, 110), f\"ID {i}\", fill=(0, 0, 0))\n    img_path = img_dir / f\"{i:04d}.png\"\n    img.save(img_path)\n\n# 4) å†™å…¥ metadata.csv\ncsv_path = root_dir / \"metadata.csv\"\nwith open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n    writer = csv.writer(f)\n    writer.writerow([\"image_path\", \"report\"])\n    for i, text in enumerate(reports, start=1):\n        writer.writerow([f\"images/{i:04d}.png\", text])\n\nprint(\"ç©å…·æ•°æ®é›†å·²ç”Ÿæˆï¼š\", root_dir)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. è¯»å–æ•°æ®å¹¶æ„å»º Dataset\n\nè¿™é‡Œæˆ‘ä»¬è‡ªå·±å†™ä¸€ä¸ª `Dataset` ç±»ï¼Œå®Œæˆä»¥ä¸‹å·¥ä½œï¼š\n- è¯»å– `metadata.csv`\n- åŠ è½½å›¾åƒ\n- è¿”å›å›¾åƒå’Œæ–‡æœ¬\n\n**è¿™ä¸€æ­¥æ˜¯è®­ç»ƒå‰æœ€é‡è¦çš„è¾“å…¥å‡†å¤‡ã€‚**\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\n\nclass MedImageTextDataset(Dataset):\n    def __init__(self, root_dir, csv_file):\n        self.root_dir = Path(root_dir)\n        self.df = pd.read_csv(self.root_dir / csv_file)\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image_path = self.root_dir / row[\"image_path\"]\n        text = row[\"report\"]\n        image = Image.open(image_path).convert(\"RGB\")\n        return image, text\n\n# è¿™é‡Œä½¿ç”¨ç©å…·æ•°æ®ï¼ˆå¦‚æœä½ æœ‰è‡ªå·±çš„æ•°æ®ï¼Œæ”¹æˆä½ çš„è·¯å¾„ï¼‰\ndataset = MedImageTextDataset(\"toy_med_data\", \"metadata.csv\")\nprint(\"æ ·æœ¬æ•°ï¼š\", len(dataset))\nprint(\"ç¤ºä¾‹ï¼š\", dataset[0])\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. åŠ è½½ CLIP æ¨¡å‹ä¸å¤„ç†å™¨\n\nCLIP éœ€è¦åŒæ—¶å¤„ç†å›¾åƒå’Œæ–‡æœ¬ï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨ `CLIPProcessor`ã€‚\n\n**é‡è¦**ï¼šè¿™é‡Œä¼šä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆéœ€è¦ç½‘ç»œï¼‰ã€‚\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import torch\nfrom transformers import CLIPProcessor, CLIPModel\n\n# é€‰æ‹©ä¸€ä¸ªè½»é‡çº§æ¨¡å‹ï¼Œé€‚åˆæ–°æ‰‹è·‘é€š\nmodel_name = \"openai/clip-vit-base-patch32\"\n\nprocessor = CLIPProcessor.from_pretrained(model_name)\nmodel = CLIPModel.from_pretrained(model_name)\n\n# å¦‚æœæœ‰ GPU å¯ä»¥åŠ é€Ÿ\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nprint(\"è®¾å¤‡ï¼š\", device)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4.1 è¿è¡Œå‰çš„å¿«é€Ÿè‡ªæ£€ï¼ˆç¡®ä¿æµç¨‹èƒ½è·‘é€šï¼‰\n\nè¿™ä¸€æ­¥åšä¸€ä¸ª **æœ€å°åŒ–çš„å‰å‘è®¡ç®—**ï¼Œç¡®è®¤ï¼š\n- æ•°æ®å¯ä»¥è¢«å¤„ç†å™¨æ­£ç¡®è¯»å–\n- æ¨¡å‹å¯ä»¥æ­£å¸¸å‰å‘æ¨ç†å¹¶è¿”å› loss\n\n> å¦‚æœè¿™ä¸€æ­¥æŠ¥é”™ï¼Œé€šå¸¸æ˜¯ç¯å¢ƒä¾èµ–æˆ–æ•°æ®è·¯å¾„é—®é¢˜ï¼Œè¯·å…ˆä¿®å¤ã€‚\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import torch\n\n# å–ä¸€ä¸ªå° batch åšè‡ªæ£€\nsample_images = [dataset[0][0], dataset[1][0]]\nsample_texts = [dataset[0][1], dataset[1][1]]\n\nwith torch.no_grad():\n    sample_inputs = processor(text=sample_texts, images=sample_images, return_tensors=\"pt\", padding=True)\n    sample_inputs = {k: v.to(device) for k, v in sample_inputs.items()}\n    sample_outputs = model(**sample_inputs, return_loss=True)\n\nprint(\"è‡ªæ£€ loss:\", sample_outputs.loss.item())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. æ„å»º DataLoader å’Œè®­ç»ƒå¾ªç¯\n\nCLIP çš„è®­ç»ƒç›®æ ‡æ˜¯ï¼š**å›¾åƒä¸æ–‡æœ¬åœ¨åŒä¸€å‘é‡ç©ºé—´é‡Œé è¿‘**ã€‚\n\næˆ‘ä»¬ä½¿ç”¨ `model(**inputs, return_loss=True)` ç›´æ¥å¾—åˆ°å¯¹æ¯”å­¦ä¹  lossã€‚\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def collate_fn(batch):\n    images, texts = zip(*batch)\n    inputs = processor(text=list(texts), images=list(images), return_tensors=\"pt\", padding=True)\n    return inputs\n\nloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n\nmodel.train()\nfor epoch in range(2):  # æ¼”ç¤ºç”¨ 2 ä¸ª epoch\n    total_loss = 0\n    for step, batch in enumerate(loader):\n        # æŠŠè¾“å…¥æ”¾åˆ°è®¾å¤‡ä¸Š\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        # è®¡ç®— loss\n        outputs = model(**batch, return_loss=True)\n        loss = outputs.loss\n\n        # åå‘ä¼ æ’­\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(loader)\n    print(f\"Epoch {epoch+1} | loss = {avg_loss:.4f}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. ç®€å•è¯„ä¼°ï¼šå›¾æ–‡åŒ¹é… Top-1\n\nè¿™é‡Œåšä¸€ä¸ªæœ€ç›´è§‚çš„è¯„ä¼°ï¼š\n- å¯¹äºæ¯å¼ å›¾ï¼Œæ‰¾æœ€ç›¸ä¼¼çš„æ–‡æœ¬\n- å¦‚æœæ‰¾åˆ°çš„æ–‡æœ¬å°±æ˜¯å®ƒåŸæœ¬é…å¯¹çš„æ–‡æœ¬ï¼Œåˆ™ç®—å¯¹\n\n> è¿™æ˜¯ä¸€ä¸ªç®€å•æ¼”ç¤ºï¼ŒçœŸå®åœºæ™¯å¯è®¡ç®—æ›´ä¸“ä¸šæŒ‡æ ‡ï¼ˆå¦‚ Recall@Kï¼‰ã€‚\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import torch.nn.functional as F\n\nmodel.eval()\n\n# å‡†å¤‡å…¨éƒ¨å›¾åƒå’Œæ–‡æœ¬\nall_images = []\nall_texts = []\nfor img, txt in dataset:\n    all_images.append(img)\n    all_texts.append(txt)\n\nwith torch.no_grad():\n    inputs = processor(text=all_texts, images=all_images, return_tensors=\"pt\", padding=True)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    outputs = model(**inputs)\n\n    image_embeds = outputs.image_embeds\n    text_embeds = outputs.text_embeds\n\n    # å½’ä¸€åŒ–åè®¡ç®—ç›¸ä¼¼åº¦\n    image_embeds = F.normalize(image_embeds, p=2, dim=-1)\n    text_embeds = F.normalize(text_embeds, p=2, dim=-1)\n\n    similarity = image_embeds @ text_embeds.T  # (N, N)\n\n# Top-1 å‡†ç¡®ç‡\ncorrect = 0\nfor i in range(similarity.size(0)):\n    pred = similarity[i].argmax().item()\n    if pred == i:\n        correct += 1\n\nacc = correct / similarity.size(0)\nprint(f\"Top-1 å›¾æ–‡åŒ¹é…å‡†ç¡®ç‡: {acc:.2f}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. æ¨ç†ç¤ºä¾‹ï¼šç»™ä¸€å¼ å›¾æ‰¾æœ€ç›¸å…³æ–‡æœ¬\n\nä¸‹é¢å±•ç¤º **å›¾åƒæ£€ç´¢æ–‡æœ¬** çš„æ•ˆæœã€‚\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# å–ç¬¬ä¸€å¼ å›¾ç‰‡è¿›è¡Œæ£€ç´¢\nquery_image = all_images[0]\n\nwith torch.no_grad():\n    inputs = processor(text=all_texts, images=[query_image], return_tensors=\"pt\", padding=True)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    outputs = model(**inputs)\n\n    image_embed = F.normalize(outputs.image_embeds, p=2, dim=-1)\n    text_embeds = F.normalize(outputs.text_embeds, p=2, dim=-1)\n    scores = (image_embed @ text_embeds.T).squeeze(0)\n\nbest_idx = scores.argmax().item()\nprint(\"æœ€ç›¸å…³æ–‡æœ¬ï¼š\", all_texts[best_idx])\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. å¦‚ä½•ç”¨è‡ªå·±çš„çœŸå®æ•°æ®ï¼ˆè¯¦ç»†æŒ‡å¯¼ï¼‰\n\n### 8.1 å‡†å¤‡è‡ªå·±çš„æ•°æ®\n1. æŠŠåŒ»å­¦å›¾åƒæ”¾åˆ° `my_med_data/images/`\n2. åˆ›å»º `my_med_data/metadata.csv`ï¼Œè‡³å°‘åŒ…å« `image_path` å’Œ `report`\n\n### 8.2 è®­ç»ƒå‰éœ€è¦ç¡®è®¤çš„ç‚¹\n- å›¾åƒæ ¼å¼æ˜¯å¦ç»Ÿä¸€ï¼ˆpng/jpgï¼‰\n- æ–‡æœ¬æ˜¯å¦æ¸…æ´—è¿‡ï¼ˆå»æ‰éšç§ä¿¡æ¯ï¼‰\n- æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿï¼ˆè¶Šå¤šè¶Šå¥½ï¼‰\n\n### 8.3 è®­ç»ƒç­–ç•¥å»ºè®®\n- **å°æ•°æ®é›†**ï¼šå»ºè®®åªå¾®è°ƒæ–‡æœ¬/å›¾åƒç¼–ç å™¨åå‡ å±‚ï¼Œé¿å…è¿‡æ‹Ÿåˆ\n- **ä¸­å¤§æ•°æ®é›†**ï¼šå¯ä»¥å…¨é‡å¾®è°ƒ\n- å­¦ä¹ ç‡å»ºè®®ä» `1e-5 ~ 5e-5` ä¹‹é—´å°è¯•\n- batch size è§†æ˜¾å­˜è€Œå®š\n\n### 8.4 ä½ çš„æ•°æ®æ¥å…¥æ–¹å¼ï¼ˆæ”¹è¿™ä¸€è¡Œï¼‰\n```python\n# æŠŠ toy_med_data æ›¿æ¢æˆä½ çš„ç›®å½•\n# dataset = MedImageTextDataset(\"my_med_data\", \"metadata.csv\")\n```\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9. å¦‚ä½•åšâ€œé¢„è®­ç»ƒâ€ vs â€œå¾®è°ƒâ€\n\n### 9.1 é¢„è®­ç»ƒï¼ˆä»é›¶å¼€å§‹ï¼‰\n- éœ€è¦å¤§é‡æ•°æ®ï¼ˆå‡ åä¸‡åˆ°ç™¾ä¸‡çº§åˆ«ï¼‰\n- è®­ç»ƒæ—¶é—´é•¿ï¼Œç®—åŠ›æ¶ˆè€—å¤§\n- å®é™…å·¥ä½œä¸­ï¼Œé€šå¸¸ **ä¸å»ºè®®æ–°æ‰‹ä»é›¶é¢„è®­ç»ƒ**\n\n### 9.2 å¾®è°ƒï¼ˆæ¨èï¼‰\n- åŸºäºç°æˆçš„ CLIP æƒé‡\n- ä½¿ç”¨ä½ è‡ªå·±çš„åŒ»å­¦æ•°æ®åšå°‘é‡è®­ç»ƒ\n- æˆæœ¬ä½ï¼Œæ•ˆæœæå‡æ˜æ˜¾\n\n> æœ¬æ•™ç¨‹å°±æ˜¯å¾®è°ƒçš„æµç¨‹ã€‚\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 10. ä¿å­˜ä¸åŠ è½½æ¨¡å‹\n\nè®­ç»ƒåè®°å¾—ä¿å­˜æ¨¡å‹ï¼Œè¿™æ ·ä¸‹æ¬¡å¯ä»¥ç›´æ¥åŠ è½½ã€‚\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ä¿å­˜\nsave_dir = \"clip_med_finetuned\"\nmodel.save_pretrained(save_dir)\nprocessor.save_pretrained(save_dir)\nprint(\"æ¨¡å‹å·²ä¿å­˜åˆ°ï¼š\", save_dir)\n\n# åŠ è½½\n# model = CLIPModel.from_pretrained(save_dir)\n# processor = CLIPProcessor.from_pretrained(save_dir)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 11. å¸¸è§é—®é¢˜ï¼ˆæ–°æ‰‹æœ€å®¹æ˜“å¡ä½çš„åœ°æ–¹ï¼‰\n\n1. **ä¸‹è½½æ¨¡å‹å¤±è´¥æ€ä¹ˆåŠï¼Ÿ**\n   - éœ€è¦è”ç½‘ï¼Œæˆ–æå‰æŠŠæ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°ã€‚\n\n2. **CPU å¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ**\n   - å°è¯•ä½¿ç”¨ GPUï¼Œæˆ–å‡å°‘ batch sizeã€‚\n\n3. **æŠ¥é”™ â€œCUDA out of memoryâ€**\n   - å‡å°‘ batch size æˆ–ä½¿ç”¨æ›´å°æ¨¡å‹ã€‚\n\n4. **å¦‚ä½•åŠ å…¥æ›´å¤šåŒ»å­¦ä»»åŠ¡ï¼Ÿ**\n   - å¯ä»¥åœ¨ CLIP embedding ååŠ ä¸€ä¸ªåˆ†ç±»å¤´ï¼Œç”¨äºç–¾ç—…åˆ†ç±»ã€‚\n\n---\n\nğŸ‰ æ­å–œä½ å®Œæˆäº†ä¸€ä¸ªå®Œæ•´çš„ CLIP å¤šæ¨¡æ€åŒ»å­¦æ•™ç¨‹ï¼\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}