{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 | 图文联合（CLIP）\n",
        "\n",
        "本 Notebook 讲解如何使用 **CLIP** 进行图文检索与对齐训练。\n",
        "我们会：\n",
        "\n",
        "1. 准备图文数据\n",
        "2. 使用 CLIP 计算相似度\n",
        "3. 做一个小型的对齐训练\n",
        "4. 评估检索效果\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. 环境准备\n",
        "\n",
        "**做什么**：安装 CLIP 相关依赖。\n",
        "**为什么**：CLIP 模型来自 Hugging Face。\n",
        "**结果**：可以直接加载预训练模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch torchvision transformers pillow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 数据准备\n",
        "\n",
        "**做什么**：准备一组图像 + 文本描述。\n",
        "**为什么**：CLIP 需要成对的图文数据。\n",
        "**结果**：得到一个小型数据集，后续可直接跑通。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "os.makedirs('data/clip', exist_ok=True)\n",
        "\n",
        "samples = [\n",
        "    ('red_lesion.png', [180, 40, 40], 'a red lesion in a medical image'),\n",
        "    ('green_tissue.png', [40, 180, 40], 'a green tissue sample'),\n",
        "    ('blue_marker.png', [40, 40, 180], 'a blue marker in the scan'),\n",
        "]\n",
        "\n",
        "for filename, color, _ in samples:\n",
        "    img = np.ones((224, 224, 3), dtype=np.uint8) * np.array(color, dtype=np.uint8)\n",
        "    Image.fromarray(img).save(os.path.join('data/clip', filename))\n",
        "\n",
        "texts = [s[2] for s in samples]\n",
        "image_paths = [os.path.join('data/clip', s[0]) for s in samples]\n",
        "print('Toy data ready:', image_paths)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 加载 CLIP 并计算相似度\n",
        "\n",
        "**做什么**：把图像和文本编码为向量，然后计算相似度。\n",
        "**为什么**：CLIP 本质上是“图文对齐”的向量空间。\n",
        "**结果**：我们可以做图文检索。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_id = 'openai/clip-vit-base-patch32'\n",
        "model = CLIPModel.from_pretrained(model_id).to(device)\n",
        "processor = CLIPProcessor.from_pretrained(model_id)\n",
        "\n",
        "images = [Image.open(p).convert('RGB') for p in image_paths]\n",
        "inputs = processor(text=texts, images=images, return_tensors='pt', padding=True)\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    image_embeds = outputs.image_embeds\n",
        "    text_embeds = outputs.text_embeds\n",
        "\n",
        "# 计算相似度矩阵\n",
        "similarity = image_embeds @ text_embeds.T\n",
        "print('Similarity matrix:\n', similarity.cpu().numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 简化版对齐训练\n",
        "\n",
        "**做什么**：使用对比损失让正确的图文配对更相似。\n",
        "**为什么**：这是 CLIP 的核心训练思想。\n",
        "**结果**：你可以在自己的医学图文数据上微调。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "model.train()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
        "\n",
        "for step in range(3):\n",
        "    inputs = processor(text=texts, images=images, return_tensors='pt', padding=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    outputs = model(**inputs)\n",
        "    image_embeds = outputs.image_embeds\n",
        "    text_embeds = outputs.text_embeds\n",
        "\n",
        "    logits = image_embeds @ text_embeds.T\n",
        "    labels = torch.arange(len(images), device=device)\n",
        "    loss_i2t = F.cross_entropy(logits, labels)\n",
        "    loss_t2i = F.cross_entropy(logits.T, labels)\n",
        "    loss = (loss_i2t + loss_t2i) / 2\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f'Step {step} Loss: {loss.item():.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 简单检索评估\n",
        "\n",
        "**做什么**：检索每个文本最匹配的图像。\n",
        "**为什么**：验证图文对齐是否有效。\n",
        "**结果**：输出最匹配的图片文件名。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "inputs = processor(text=texts, images=images, return_tensors='pt', padding=True)\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    image_embeds = outputs.image_embeds\n",
        "    text_embeds = outputs.text_embeds\n",
        "\n",
        "similarity = image_embeds @ text_embeds.T\n",
        "best_matches = similarity.argmax(dim=0).cpu().numpy()\n",
        "\n",
        "for i, text in enumerate(texts):\n",
        "    matched_path = image_paths[best_matches[i]]\n",
        "    print(f'Text: {text} -> Image: {matched_path}')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
